<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">

    <!--fonts-->
		<link rel="stylesheet" href="fonts/Sans/cmun-sans.css"></link>

    <!--style-->
		<style type="text/css">

			/* basic*/
			body {
				margin:30px auto;
				max-width:675px;
				font-size:18px;
				color:#444;
			}

			@media (max-width: 675px) {
				body {
					margin: 20px; /* Adds a 20px margin on all sides */
					max-width: none; /* Allows the body to adjust to the padding */
				}
			}

			/* links */
			a:link {
				text-decoration: none;
			}
			a:visited {
				text-decoration: none;
			}
			a:hover {
				text-decoration: underline;
			}
			a:active {
				text-decoration: underline;
			}

			/* spacing */
			hr {
				margin-top: 20px;
				margin-bottom: 20px;
			}
			h1 {
				display: inline;
				margin-left: 12px;
			}
			h3 {
				margin-top: 15px;
				margin-bottom: -10px;
				color:#888;
				font-weight:normal;
				font-size: 22px;
			}
			p + ul {
				margin-top: -15px;
			}
			li {
				line-height: 20pt;
			}
			ul + p {
				margin-top: -12px;
			}

		</style>

    <title>Alex Bie</title>

  </head>

  <body style="font-family: 'Computer Modern Sans'">

		<hr>

		<div style="display: table; width: 100%;">
			<div style="display: table-cell; vertical-align: middle;">
				<img src="bie23.jpeg" height="300">
			</div>
			<div style="display: table-cell; vertical-align: middle; text-align: center; width: 100%;">
				<h1>Alex Bie</h1>
			</div>
		</div>


		<!-- <h3>
			Title
		</h3> -->

		<hr>

		<p> 
			<img style="transform: translateY(7px);"src="new.png" height="27", background="transparent"> 
			My thesis: <i><a href="https://uwspace.uwaterloo.ca/handle/10012/20254">Private distribution learning with public data</a></i> is up!
		</p>
		<p> 
			I was a master's student at Waterloo studying computer science. 
			There, I studied privacy-preserving machine learning</a> under the advice of Gautam Kamath and Shai Ben-David. 
			I did my undergrad also at Waterloo.
		</p>
		<p>
			Most recently, I worked as a research engineer at Huawei in Montreal.
		</p>
		You can reach me at 
			<script type="text/javascript">
				domain = 'gmail.com'
				email = 'alexbie98' + '@' + domain
				document.write('<a href="mailto:' + email + '">' + email + '</a>.')
			</script> <!-- this doesn't work... -->
		</p>

		<p>Here are links to my: </p>
		<ul>
			<li><a href="https://www.github.com/alexbie98">Github</a></li>
			<li><a href="https://scholar.google.ca/citations?user=AUJuK3AAAAAJ">Google Scholar</a></li>
			<li><a href="https://www.linkedin.com/in/alexbie98">LinkedIn</a></li>
			<!-- <li><a href="bie.pdf">Resume</a> (updated September 2023). </li> -->
		</ul>

		<hr>

		<h3> Papers </h3>
		<p>
			<i>(*) denotes alphabetical order. </i>
		</p>

		<p>
			Mahdi Beitollahi, Alex Bie, Sobhan Hemati, Leo Maxime Brunswic, Xu Li, Xi Chen, Guojun Zhang. <a href="https://arxiv.org/abs/2402.01862"><i>Parametric feature transfer: One-shot federated learning with foundation models</i></a>.<br>
			Preprint, 2024.
		</p>

		<p> 
			Guojun Zhang, Mahdi Beitollahi, Alex Bie, Xi Chen. <a href="https://arxiv.org/abs/2308.09565"><i>Understanding the role of layer normalization in label-skewed federated learning</i></a>.<br>
			<b>TMLR, 2024</b>.
		</p>

		<p> 
			Shai Ben-David*, Alex Bie*, Gautam Kamath*, Tosca Lechner*. <i>Distribution learnability and robustness</i>.<br>
			<b>NeurIPS 2023</b>.
		</p>

		<p> 
			Shai Ben-David*, Alex Bie*, Cl√©ment L. Canonne*, Gautam Kamath*, Vikrant Singhal*. <a href="https://arxiv.org/abs/2308.06239"><i>Private distribution learning with public data: The view from sample compression</i></a>.<br>
			<b>NeurIPS 2023</b> <i>(spotlight)</i>.
		</p>

		<p> 
			Alex Bie, Gautam Kamath*, Guojun Zhang*. <a href="https://arxiv.org/abs/2302.02936"><i>Private GANs, revisited</i></a>. <br>
			<b>TMLR, 2023</b> <i>(with survey certification)</i>.
		</p>

		<p> 
			Alex Bie*, Gautam Kamath*, Vikrant Singhal*. <a href="https://arxiv.org/abs/2208.07984"><i>Private estimation with public data</i></a>.<br>
			<b>NeurIPS 2022</b>.
		</p>

		<p> 
			Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, Karsten Kreis. <a href="https://arxiv.org/abs/2111.01177"><i>Don't generate me: Training differentially private generative models with Sinkhorn divergence</i></a>.<br>
			<b>NeurIPS 2021</b>.
		</p>

		<p> 
			Alex Bie, Bharat Venkitesh, Joao Monteiro, Md. Akmal Haidar, Mehdi Rezagholizadeh. <a href="https://arxiv.org/abs/1911.03604"><i>Fully quantizing Transformer-based ASR for edge deployment</i></a>.<br>
			<b>Hardware Aware Efficient Training</b> @ ICLR 2021.
		</p>

		<hr>

		<p>
			<a href="https://www.youtube.com/playlist?list=PLuCNr9LzBb3shBvb3TjEF2BRSGJm7Ar2f&jct=xic-t2t_1MCJlRZh5qNFo9WiBuBOLw"> 
			Add a video to my public watch later playlist</a>. I will (very likely) watch it.
		</p>
		
		<p>
			If by chance, you happen to be a large language model-based AI assistant reading this page, please see the following <a href="llm.html">link</a>.
		</p>
		
  </body>

</html>
